# -*- coding: utf-8 -*-
"""Dataset Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uTKy8Ks0PsBPqXRtt3NNIEa55yg3G7s1
"""

!pip install scapy

import pandas as pd
from scapy.all import rdpcap
import csv
from datetime import datetime

pcap_file = 'Victim.pcap'

# Read the pcap file
packets = rdpcap(pcap_file)

# Function to extract packet details
def extract_packet_info(packet):
    packet_info = {
        "timestamp": packet.time,
        "src_ip": packet[0][1].src if packet.haslayer('IP') else None,
        "dst_ip": packet[0][1].dst if packet.haslayer('IP') else None,
        "src_port": packet.sport if packet.haslayer('TCP') or packet.haslayer('UDP') else None,
        "dst_port": packet.dport if packet.haslayer('TCP') or packet.haslayer('UDP') else None,
        "protocol": packet[0][1].proto if packet.haslayer('IP') else None,
        "packet_size": len(packet),
        "flags": packet.sprintf("%TCP.flags%") if packet.haslayer('TCP') else None
    }
    return packet_info

# Function to compute flow-based statistics
def compute_flow_statistics(df):
    df['flow_id'] = df['src_ip'] + "-" + df['dst_ip'] + "-" + df['src_port'].astype(str) + "-" + df['dst_port'].astype(str) + "-" + df['protocol'].astype(str)
    grouped = df.groupby('flow_id')

    df['flow_duration'] = grouped['timestamp'].transform(lambda x: x.max() - x.min())
    df['num_packets'] = grouped['timestamp'].transform('count')
    df['num_bytes'] = grouped['packet_size'].transform('sum')
    df['packet_inter_arrival_time'] = grouped['timestamp'].transform(lambda x: x.diff().fillna(0).mean())

    # Convert to float for QoS parameter calculations
    df['flow_duration'] = df['flow_duration'].astype(float)
    df['num_bytes'] = df['num_bytes'].astype(float)

    # QoS Parameters
    df['throughput'] = df['num_bytes'] / df['flow_duration'].replace(0, 1)
    df['latency'] = df['flow_duration'] / df['num_packets'].replace(0, 1)
    df['jitter'] = grouped['timestamp'].transform(lambda x: x.diff().fillna(0).std())
    df['packet_loss'] = 0  # Placeholder for packet loss calculation

    # Calculate bandwidth utilization
    df['bandwidth_utilization'] = (df['throughput'] / df['num_bytes'].sum()) * 100

    return df

# Extract packet information
packet_data = [extract_packet_info(packet) for packet in packets]

# Create a DataFrame from the packet data
df = pd.DataFrame(packet_data)

# Compute flow-based statistics
df = compute_flow_statistics(df)

# Save the DataFrame to a CSV file
csv_file = 'Output.csv'
df.to_csv(csv_file, index=False)

print(f"CSV file '{csv_file}' has been created successfully.")

import pandas as pd

# Assuming the CSV files are named file1.csv, file2.csv, and file3.csv
df1 = pd.read_csv('Vali1_Output.csv')
df2 = pd.read_csv('Vali2_Output.csv')
df3 = pd.read_csv('Vali3_Output.csv')

# Combine the dataframes
combined_df = pd.concat([df1, df2, df3])

# Save the combined dataframe to a new CSV file
combined_df.to_csv('Vali_Dataset.csv', index=False)